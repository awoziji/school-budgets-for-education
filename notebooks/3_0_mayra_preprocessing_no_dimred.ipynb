{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_0_mayra_preprocessing_no_dimred.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "ZsrhZTaxKEQj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.pipeline import FeatureUnion, Pipeline\n",
        "from sklearn.preprocessing import Imputer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from warnings import warn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AhRExPjIKk0Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Read the data using the Unnamed (probably id) as index\n",
        "url = 'https://s3.amazonaws.com/drivendata/data/4/public/81e8f2de-9915-4934-b9ae-9705685c9d50.csv'\n",
        "#url = '../src/data/raw/training.csv'\n",
        "training = pd.read_csv(url, index_col='Unnamed: 0')\n",
        "\n",
        "labels = ['Function', 'Object_Type', 'Operating_Status', 'Position_Type', 'Pre_K', 'Reporting', \n",
        "          'Sharing', 'Student_Type', 'Use']\n",
        "\n",
        "numeric = ['FTE', 'Total']\n",
        "\n",
        "categoric = [ 'Facility_or_Department', 'Function_Description', \n",
        "            'Fund_Description', 'Job_Title_Description', 'Location_Description', \n",
        "            'Object_Description', 'Position_Extra', 'Program_Description', 'SubFund_Description', \n",
        "            'Sub_Object_Description', \n",
        "            'Text_1', 'Text_2', 'Text_3', 'Text_4']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J0YX_TbB74o0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Grouping the data by the Object_Type will help to calculate the median of each group on the imputation step."
      ]
    },
    {
      "metadata": {
        "id": "X-3VFrioPSFw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "groupped_FTE = training[['FTE', 'Object_Type']].groupby(by='Object_Type')\n",
        "groupped_total = training[['Total', 'Object_Type']].groupby(by='Object_Type')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vpbmr1_LJ9RX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### FunctionTransformers"
      ]
    },
    {
      "metadata": {
        "id": "gd4Nfma5J0n0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define combine_text_columns()\n",
        "def combine_text_columns(data_frame):\n",
        "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
        "    \n",
        "    # Drop non-text columns that are in the df\n",
        "    text_data = data_frame[categoric]\n",
        "    \n",
        "    # Replace nans with blanks\n",
        "    text_data.fillna(\"\", inplace=True)\n",
        "    \n",
        "    for category in categoric:\n",
        "      training[category] = training[category].str.lower()\n",
        "    \n",
        "    \n",
        "    # Join all text items in a row that have a space in between\n",
        "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zKQx45RdKR4o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define combine_numeric_columns()\n",
        "def combine_numeric_columns(data_frame, groupped_FTE=groupped_FTE, groupped_total=groupped_total):\n",
        "    \"\"\" process all the numeric data \"\"\"\n",
        "    \n",
        "    # Drop non-numeric columns that are in the df\n",
        "    data = data_frame[numeric]\n",
        "    \n",
        "    #Remove inconsistent data\n",
        "    data[(data[numeric[0]] < 0) | (data[numeric[1]] < 0) ] = np.nan\n",
        "    \n",
        "    #Impute the missing data with the median from each class\n",
        "    for group in groupped.index:\n",
        "      indexes_FTE = groupped_FTE.get_group(group).index.values\n",
        "      indexes_total = groupped_total.get_group(group).index.values\n",
        "      data.loc[ data.FTE.isnull() & np.isin(data.index.values,indexes_FTE), 'FTE'] = groupped_FTE.median().loc[group, \"FTE\"]\n",
        "      data.loc[ data.Total.isnull() & np.isin(data.index.values,indexes_total), 'Total'] = groupped_total.median().loc[group,\"Total\"]\n",
        "      \n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k1BlfwlPKWnC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Preprocess the text data: get_text_data\n",
        "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
        "\n",
        "# Preprocess the numeric data: get_numeric_data\n",
        "get_numeric_data = FunctionTransformer(combine_numeric_columns, validate=False)\n",
        "\n",
        "\n",
        "\n",
        "def multilabel_sample(y, size=1000, min_count=5, seed=None):\n",
        "    \"\"\" Takes a matrix of binary labels `y` and returns\n",
        "        the indices for a sample of size `size` if\n",
        "        `size` > 1 or `size` * len(y) if size =< 1.\n",
        "\n",
        "        The sample is guaranteed to have > `min_count` of\n",
        "        each label.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if (np.unique(y).astype(int) != np.array([0, 1])).any():\n",
        "            raise ValueError()\n",
        "    except (TypeError, ValueError):\n",
        "        raise ValueError('multilabel_sample only works with binary indicator matrices')\n",
        "\n",
        "    if (y.sum(axis=0) < min_count).any():\n",
        "        raise ValueError('Some classes do not have enough examples. Change min_count if necessary.')\n",
        "\n",
        "    if size <= 1:\n",
        "        size = np.floor(y.shape[0] * size)\n",
        "\n",
        "    if y.shape[1] * min_count > size:\n",
        "        msg = \"Size less than number of columns * min_count, returning {} items instead of {}.\"\n",
        "        warn(msg.format(y.shape[1] * min_count, size))\n",
        "        size = y.shape[1] * min_count\n",
        "\n",
        "    rng = np.random.RandomState(seed if seed is not None else np.random.randint(1))\n",
        "\n",
        "    if isinstance(y, pd.DataFrame):\n",
        "        choices = y.index\n",
        "        y = y.values\n",
        "    else:\n",
        "        choices = np.arange(y.shape[0])\n",
        "\n",
        "    sample_idxs = np.array([], dtype=choices.dtype)\n",
        "\n",
        "    # first, guarantee > min_count of each label\n",
        "    for j in range(y.shape[1]):\n",
        "        label_choices = choices[y[:, j] == 1]\n",
        "        label_idxs_sampled = rng.choice(label_choices, size=min_count, replace=False)\n",
        "        sample_idxs = np.concatenate([label_idxs_sampled, sample_idxs])\n",
        "\n",
        "    sample_idxs = np.unique(sample_idxs)\n",
        "\n",
        "    # now that we have at least min_count of each, we can just random sample\n",
        "    sample_count = int(size - sample_idxs.shape[0])\n",
        "\n",
        "    # get sample_count indices from remaining choices\n",
        "    remaining_choices = np.setdiff1d(choices, sample_idxs)\n",
        "    remaining_sampled = rng.choice(remaining_choices,\n",
        "                                   size=sample_count,\n",
        "                                   replace=False)\n",
        "\n",
        "    return np.concatenate([sample_idxs, remaining_sampled])\n",
        "\n",
        "\n",
        "def multilabel_sample_dataframe(df, labels, size, min_count=5, seed=None):\n",
        "    \"\"\" Takes a dataframe `df` and returns a sample of size `size` where all\n",
        "        classes in the binary matrix `labels` are represented at\n",
        "        least `min_count` times.\n",
        "    \"\"\"\n",
        "    idxs = multilabel_sample(labels, size=size, min_count=min_count, seed=seed)\n",
        "    return df.loc[idxs]\n",
        "\n",
        "\n",
        "def multilabel_train_test_split(X, Y, size, min_count=5, seed=None):\n",
        "    \"\"\" Takes a features matrix `X` and a label matrix `Y` and\n",
        "        returns (X_train, X_test, Y_train, Y_test) where all\n",
        "        classes in Y are represented at least `min_count` times.\n",
        "    \"\"\"\n",
        "    index = Y.index if isinstance(Y, pd.DataFrame) else np.arange(Y.shape[0])\n",
        "\n",
        "    test_set_idxs = multilabel_sample(Y, size=size, min_count=min_count, seed=seed)\n",
        "    train_set_idxs = np.setdiff1d(index, test_set_idxs)\n",
        "\n",
        "    test_set_mask = index.isin(test_set_idxs)\n",
        "    train_set_mask = ~test_set_mask\n",
        "\n",
        "    return (X[train_set_mask], X[test_set_mask], Y[train_set_mask], Y[test_set_mask])\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kP0v3M2_5DH1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Recover the targets and spli the data\n",
        "y = pd.get_dummies(training['Object_Type'])  \n",
        "X_train, X_test, y_train, y_test = multilabel_train_test_split(training.drop(columns=labels), \n",
        "                                                    y, \n",
        "                                                    size=0.33, seed=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yfHLD6La5GAY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pipeline\n",
        "\n",
        "Apply the transformations on numeric and categorica data. No dimension reduction is used."
      ]
    },
    {
      "metadata": {
        "id": "fF2c4SLDhTGI",
        "colab_type": "code",
        "outputId": "5a4153e5-769f-4147-a7f9-a2d43a5d97db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imp', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer',HashingVectorizer(token_pattern=\"[A-Za-z0-9]+(?=\\\\s+)\", \n",
        "                                                    norm=None, \n",
        "                                                    binary=False,\n",
        "                                                    ngram_range=(1,2)) \n",
        "                    )\n",
        "                ]))\n",
        "             ]\n",
        "        ))\n",
        "        \n",
        "    ])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "8i9c6MQV6xoE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Applying the steps, we got a sparse matrix with 1048578 features."
      ]
    },
    {
      "metadata": {
        "id": "VTsglZWNjFFo",
        "colab_type": "code",
        "outputId": "7d365f78-ff9e-4d6f-f3fa-6ea01f4b2867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "cell_type": "code",
      "source": [
        "preprocessed = pl.fit_transform(X_train, y_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:2534: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  self.loc._setitem_with_indexer(indexer, value)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:2514: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  self._setitem_array(key, value)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:194: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  self._setitem_with_indexer(indexer, value)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3035: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  downcast=downcast, **kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "BbHm54kyjgPn",
        "colab_type": "code",
        "outputId": "3e19b97a-b263-4ffb-9861-51f2ddd145e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "type(preprocessed)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse.csr.csr_matrix"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "x1ipkqS14x96",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "de888024-c929-4bae-f978-8716d78a9841"
      },
      "cell_type": "code",
      "source": [
        "preprocessed[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x1048578 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 10 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "09avNYUp4zw5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}